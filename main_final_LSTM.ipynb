{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-10T10:17:23.018104Z",
     "start_time": "2025-11-10T10:17:23.012051Z"
    }
   },
   "source": [
    "\n",
    "# Core data manipulation libraries\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np  # For numerical operations and array functionality\n",
    "import re  # For regular expressions (text pattern matching)\n",
    "\n",
    "# NLTK (Natural Language Toolkit) imports for text processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize  # Split text into individual words\n",
    "from nltk.corpus import stopwords  # Common words to filter out (e.g., 'the', 'a', 'is')\n",
    "from nltk.stem import PorterStemmer  # Reduce words to their root form (stem)\n",
    "from nltk.stem import WordNetLemmatizer  # Reduce words to their dictionary form (lemma)\n",
    "\n",
    "# Scikit-learn Pipeline for chaining preprocessing and model steps\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Scikit-learn feature extraction tools for text vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # Convert text to word count vectors\n",
    "from sklearn.feature_extraction.text import TfidfTransformer  # Transform counts to TF-IDF representation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Combined CountVectorizer + TfidfTransformer\n",
    "\n",
    "# Scikit-learn classification models\n",
    "from sklearn.neighbors import KNeighborsClassifier  # K-Nearest Neighbors classifier\n",
    "from sklearn.naive_bayes import MultinomialNB  # Naive Bayes classifier for text\n",
    "from sklearn.svm import SVC  # Support Vector Machine classifier\n",
    "\n",
    "# Scikit-learn model selection and validation tools\n",
    "from sklearn.model_selection import StratifiedKFold  # Stratified K-fold cross-validation (maintains class distribution)\n",
    "from sklearn.model_selection import KFold  # Standard K-fold cross-validation\n",
    "from sklearn.model_selection import train_test_split  # Split data into train/test sets\n",
    "\n",
    "# Scikit-learn evaluation metrics\n",
    "from sklearn.metrics import accuracy_score  # Calculate accuracy percentage\n",
    "from sklearn.metrics import precision_score  # Calculate precision (true positives / predicted positives)\n",
    "from sklearn.metrics import recall_score  # Calculate recall (true positives / actual positives)\n",
    "from sklearn.metrics import f1_score  # Calculate F1 score (harmonic mean of precision and recall)\n",
    "from sklearn.metrics import classification_report  # Generate comprehensive classification metrics\n",
    "\n",
    "# Scikit-learn base classes for creating custom transformers\n",
    "from sklearn.base import BaseEstimator, TransformerMixin  # Base classes for custom pipeline components\n",
    "\n",
    "# Gensim for word embeddings\n",
    "from gensim.models import Word2Vec  # Train and use Word2Vec word embedding models\n",
    "\n",
    "# TensorFlow/Keras for deep learning models\n",
    "from tensorflow.keras.models import Sequential  # Sequential neural network model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding  # Neural network layer types\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # Convert text to sequences of integers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  # Pad sequences to uniform length\n",
    "\n",
    "# Download required NLTK data files\n",
    "nltk.download('punkt')  # Tokenizer models for sentence and word splitting\n",
    "nltk.download('stopwords')  # Lists of common stopwords in multiple languages\n",
    "nltk.download('wordnet')\n",
    "# Ignore warnings to make life easier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lapos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lapos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lapos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:17:27.907391Z",
     "start_time": "2025-11-10T10:17:23.035667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_train = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/plain_text/train-00000-of-00001.parquet\")\n",
    "df_test = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/plain_text/test-00000-of-00001.parquet\")\n",
    "df_unsupervised = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/plain_text/unsupervised-00000-of-00001.parquet\")"
   ],
   "id": "afe10b6ff7e72d83",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:17:27.956684Z",
     "start_time": "2025-11-10T10:17:27.951718Z"
    }
   },
   "cell_type": "code",
   "source": "df_train.shape",
   "id": "ca1929ecec8c9095",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:17:27.984966Z",
     "start_time": "2025-11-10T10:17:27.973615Z"
    }
   },
   "cell_type": "code",
   "source": "df_test.shape",
   "id": "6b93ed73ca622730",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:17:28.003820Z",
     "start_time": "2025-11-10T10:17:27.992297Z"
    }
   },
   "cell_type": "code",
   "source": "df_train.columns\n",
   "id": "46d2b88c3f3a2822",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'label'], dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:17:28.021751Z",
     "start_time": "2025-11-10T10:17:28.013159Z"
    }
   },
   "cell_type": "code",
   "source": "df_test.columns\n",
   "id": "8effb28e45c472f3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'label'], dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:17:28.055965Z",
     "start_time": "2025-11-10T10:17:28.051348Z"
    }
   },
   "cell_type": "code",
   "source": "df_train['label'].unique()",
   "id": "a8ff46924f8575c2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:17:28.087574Z",
     "start_time": "2025-11-10T10:17:28.083067Z"
    }
   },
   "cell_type": "code",
   "source": "df_test['label'].unique()",
   "id": "b0ec880846bb46c0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:17:28.113318Z",
     "start_time": "2025-11-10T10:17:28.108508Z"
    }
   },
   "cell_type": "code",
   "source": "df_train['text'].sample(10)",
   "id": "e7dcacbc14885b77",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14149    Fr√§ulein Doktor is as good a demonstration as ...\n",
       "20683    Magnificent and unforgettable, stunningly atmo...\n",
       "9186     ... or maybe it just IS this bad. The plot is ...\n",
       "15120    Finally, Timon and Pumbaa in their own film......\n",
       "13334    I will admit, I thought this movie wasn't goin...\n",
       "8944     This U.S soap opera, 'Knots Landing' has all t...\n",
       "12386    I am sorry to fans of this film but it is the ...\n",
       "3746     Err...this movie sucked. A LOT.<br /><br />I h...\n",
       "20366    The Write Word<br /><br />What you see is what...\n",
       "18116    I sat through both parts of Che last night, ba...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:17:28.186573Z",
     "start_time": "2025-11-10T10:17:28.181388Z"
    }
   },
   "cell_type": "code",
   "source": "df_test['text'].sample(10)",
   "id": "b6062f3f48dd106a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7319     What a bloody nuisance! You can't get on subje...\n",
       "18728    For some perverse reason best known to themsel...\n",
       "8340     I sat last night to see this film being played...\n",
       "8839     This TV show is possibly the most pathetic dis...\n",
       "10238    I don't even understand what they tried to acc...\n",
       "13416    So keira knightly is in it...So automatically ...\n",
       "23266    I started watching this expecting the worst, i...\n",
       "10118    I think my summary says it all. This MTV-ish a...\n",
       "3100     I think Homegrown is a bit of a misnomer for t...\n",
       "4944     weak direction, weak plot, unimpressive music,...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:17:28.347832Z",
     "start_time": "2025-11-10T10:17:28.341658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class pre_process_text(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        prep_sentences = []\n",
    "        for text in X:\n",
    "            # Remove HTML tags (good practice)\n",
    "            text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "            # Tokenize\n",
    "            tokens = word_tokenize(text)\n",
    "\n",
    "            words_to_keep = {\"not\", \"no\", \"against\", \"down\",\"no\", \"nor\", \"not\", \"don\",\"very\", \"too\", \"more\", \"most\", \"so\", \"only\"}\n",
    "\n",
    "\n",
    "            for word in words_to_keep:\n",
    "                self.stop_words.discard(word)\n",
    "\n",
    "            # Process tokens\n",
    "            processed = [\n",
    "                self.lemmatizer.lemmatize(token.lower())\n",
    "                for token in tokens\n",
    "                if token.isalpha() and token.lower() not in self.stop_words\n",
    "            ]\n",
    "\n",
    "            # Join back to a string\n",
    "            prep_sentences.append(\" \".join(processed))\n",
    "\n",
    "        return prep_sentences\n",
    "\n",
    "class Word2VecAverager(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, w2v_model):\n",
    "        self.w2v_model = w2v_model\n",
    "        self.vector_size = w2v_model.wv.vector_size\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        avg_vectors = []\n",
    "\n",
    "        for doc in X:\n",
    "            doc_vectors = []\n",
    "            # We .split() the processed string\n",
    "            for word in doc.split():\n",
    "                if word in self.w2v_model.wv:\n",
    "                    doc_vectors.append(self.w2v_model.wv[word])\n",
    "\n",
    "            if not doc_vectors:\n",
    "                avg_vectors.append(np.zeros(self.vector_size))\n",
    "            else:\n",
    "                avg_vectors.append(np.mean(doc_vectors, axis=0))\n",
    "\n",
    "        return np.array(avg_vectors)\n"
   ],
   "id": "d59225a3741ea4c",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:17:28.427630Z",
     "start_time": "2025-11-10T10:17:28.417296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_train = pd.DataFrame(df_train)\n",
    "df_test = pd.DataFrame(df_test)\n",
    "\n",
    "X_train = df_train['text']\n",
    "y_train = df_train['label'].values\n",
    "X_test = df_test['text']\n",
    "y_test = df_test['label'].values"
   ],
   "id": "de728b1170ec0dc2",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-10T11:28:21.384566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras.src.callbacks import EarlyStopping\n",
    "from keras.src.layers import Bidirectional, Dropout\n",
    "\n",
    "# --- Pre-process all text data ---\n",
    "print(\"--- Preprocessing Text ---\")\n",
    "pre_processor = pre_process_text()\n",
    "\n",
    "# Fit on training data and transform both train and test\n",
    "X_train = pre_processor.fit_transform(X_train)\n",
    "X_test = pre_processor.transform(X_test)\n",
    "print(f\"Example processed review: {X_train[0][:150]}...\")\n",
    "\n",
    "tokenized_processed_train = [review.split() for review in X_train]\n",
    "\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "\n",
    "min_word_count = 3\n",
    "\n",
    "\n",
    "window_size = 5\n",
    "\n",
    "\n",
    "num_workers = 4\n",
    "\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=tokenized_processed_train,\n",
    "    vector_size=embedding_dim,\n",
    "    window=window_size,\n",
    "    min_count=min_word_count,\n",
    "    workers=num_workers,\n",
    "    sg=1,       # Use Skip-gram\n",
    "    negative=5  # Use Negative Sampling\n",
    ")\n",
    "\n",
    "keras_tokenizer = Tokenizer()\n",
    "keras_tokenizer.fit_on_texts(X_train) # Fit on the processed text\n",
    "X_train_sequences = keras_tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "\n",
    "all_lengths = [len(seq) for seq in X_train_sequences]\n",
    "# max_length = int(np.percentile(all_lengths, 95))\n",
    "max_length = max(len(sentence.split())\n",
    "                for text in df_train['text']\n",
    "                for sentence in text.split('.'))\n",
    "\n",
    "print(max_length)\n",
    "\n",
    "X_train_padded = pad_sequences(\n",
    "    X_train_sequences,\n",
    "    maxlen=max_length,\n",
    "    padding='post'\n",
    ")\n",
    "\n",
    "\n",
    "vocab_size = len(keras_tokenizer.word_index) + 1\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in keras_tokenizer.word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]\n",
    "\n",
    "\n",
    "def build_lstm_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=max_length,  # Use calculated max_length\n",
    "        trainable=False  # Freeze the embeddings\n",
    "    ))\n",
    "    # Use Bidirectional LSTM\n",
    "    model.add(Bidirectional(LSTM(\n",
    "        units=100,\n",
    "        recurrent_dropout=0.2  # Add recurrent dropout\n",
    "    )))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "early_stopper = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True  # Critically important!\n",
    ")\n",
    "n_splits = 5\n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "acc_scores, f1_scores, precision_scores, recall_scores = [], [], [], []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_padded, y_train)):\n",
    "    print(f\"\\n--- Fold {fold + 1}/{n_splits} ---\")\n",
    "    X_fold_train = X_train_padded[train_idx]\n",
    "    y_fold_train = y_train[train_idx]\n",
    "    X_fold_val = X_train_padded[val_idx]\n",
    "    y_fold_val = y_train[val_idx]\n",
    "\n",
    "    model = build_lstm_model()\n",
    "    print(f\"Training on {len(X_fold_train)} samples, validating on {len(X_fold_val)} samples.\")\n",
    "    model.fit(\n",
    "        X_fold_train, y_fold_train,\n",
    "        epochs=20,\n",
    "        callbacks=[early_stopper],\n",
    "        validation_data=(X_fold_val, y_fold_val),\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    y_pred_probs = model.predict(X_fold_val)\n",
    "    y_pred_classes = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "    acc = accuracy_score(y_fold_val, y_pred_classes)\n",
    "    precision = precision_score(y_fold_val, y_pred_classes, zero_division=0)\n",
    "    recall = recall_score(y_fold_val, y_pred_classes, zero_division=0)\n",
    "    f1 = f1_score(y_fold_val, y_pred_classes, zero_division=0)\n",
    "\n",
    "    acc_scores.append(acc)\n",
    "    f1_scores.append(f1)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "\n",
    "    print(f\"Fold {fold + 1} Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_fold_val, y_pred_classes, zero_division=0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "final_model = build_lstm_model()\n",
    "final_model.fit(\n",
    "    X_train_padded,  # All processed, padded training data\n",
    "    y_train,         # All training labels\n",
    "    epochs=20,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Evaluating on Test Set...\")\n",
    "X_test_sequences = keras_tokenizer.texts_to_sequences(X_test) # Use processed X_test\n",
    "X_test_padded = pad_sequences(\n",
    "    X_test_sequences,\n",
    "    maxlen=max_length, # Use same max_length\n",
    "    padding='post'\n",
    ")\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_pred_probs = final_model.predict(X_test_padded)\n",
    "test_pred_classes = (test_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "print(\"\\n--- Test Set Performance ---\")\n",
    "test_acc = accuracy_score(y_test, test_pred_classes)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(classification_report(y_test, test_pred_classes, zero_division=0))"
   ],
   "id": "6fbb8d0f55d12fa8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preprocessing Text ---\n",
      "Example processed review: rented video store controversy surrounded first released also heard first seized custom ever tried enter country therefore fan film considered controv...\n",
      "645\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "Training on 20000 samples, validating on 5000 samples.\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "881489a78c75ba51"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
