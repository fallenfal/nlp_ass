{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-10T12:17:46.631657Z",
     "start_time": "2025-11-10T12:17:46.331113Z"
    }
   },
   "source": [
    "\n",
    "# Core data manipulation libraries\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np  # For numerical operations and array functionality\n",
    "import re  # For regular expressions (text pattern matching)\n",
    "\n",
    "# NLTK (Natural Language Toolkit) imports for text processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize  # Split text into individual words\n",
    "from nltk.corpus import stopwords  # Common words to filter out (e.g., 'the', 'a', 'is')\n",
    "from nltk.stem import PorterStemmer  # Reduce words to their root form (stem)\n",
    "from nltk.stem import WordNetLemmatizer  # Reduce words to their dictionary form (lemma)\n",
    "\n",
    "# Scikit-learn Pipeline for chaining preprocessing and model steps\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Scikit-learn feature extraction tools for text vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # Convert text to word count vectors\n",
    "from sklearn.feature_extraction.text import TfidfTransformer  # Transform counts to TF-IDF representation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Combined CountVectorizer + TfidfTransformer\n",
    "\n",
    "# Scikit-learn classification models\n",
    "from sklearn.neighbors import KNeighborsClassifier  # K-Nearest Neighbors classifier\n",
    "from sklearn.naive_bayes import MultinomialNB  # Naive Bayes classifier for text\n",
    "from sklearn.svm import SVC  # Support Vector Machine classifier\n",
    "\n",
    "# Scikit-learn model selection and validation tools\n",
    "from sklearn.model_selection import StratifiedKFold  # Stratified K-fold cross-validation (maintains class distribution)\n",
    "from sklearn.model_selection import KFold  # Standard K-fold cross-validation\n",
    "from sklearn.model_selection import train_test_split  # Split data into train/test sets\n",
    "\n",
    "# Scikit-learn evaluation metrics\n",
    "from sklearn.metrics import accuracy_score  # Calculate accuracy percentage\n",
    "from sklearn.metrics import precision_score  # Calculate precision (true positives / predicted positives)\n",
    "from sklearn.metrics import recall_score  # Calculate recall (true positives / actual positives)\n",
    "from sklearn.metrics import f1_score  # Calculate F1 score (harmonic mean of precision and recall)\n",
    "from sklearn.metrics import classification_report  # Generate comprehensive classification metrics\n",
    "\n",
    "# Scikit-learn base classes for creating custom transformers\n",
    "from sklearn.base import BaseEstimator, TransformerMixin  # Base classes for custom pipeline components\n",
    "\n",
    "# Gensim for word embeddings\n",
    "from gensim.models import Word2Vec  # Train and use Word2Vec word embedding models\n",
    "\n",
    "# TensorFlow/Keras for deep learning models\n",
    "from tensorflow.keras.models import Sequential  # Sequential neural network model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding  # Neural network layer types\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # Convert text to sequences of integers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  # Pad sequences to uniform length\n",
    "\n",
    "# Download required NLTK data files\n",
    "nltk.download('punkt')  # Tokenizer models for sentence and word splitting\n",
    "nltk.download('stopwords')  # Lists of common stopwords in multiple languages\n",
    "nltk.download('wordnet')\n",
    "# Ignore warnings to make life easier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lapos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lapos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lapos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T12:17:53.454665Z",
     "start_time": "2025-11-10T12:17:46.637492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_train = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/plain_text/train-00000-of-00001.parquet\")\n",
    "df_test = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/plain_text/test-00000-of-00001.parquet\")\n",
    "df_unsupervised = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/plain_text/unsupervised-00000-of-00001.parquet\")"
   ],
   "id": "afe10b6ff7e72d83",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T12:17:53.504947Z",
     "start_time": "2025-11-10T12:17:53.501554Z"
    }
   },
   "cell_type": "code",
   "source": "df_train.shape",
   "id": "ca1929ecec8c9095",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T12:17:53.540870Z",
     "start_time": "2025-11-10T12:17:53.519326Z"
    }
   },
   "cell_type": "code",
   "source": "df_test.shape",
   "id": "6b93ed73ca622730",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T12:17:53.558587Z",
     "start_time": "2025-11-10T12:17:53.548114Z"
    }
   },
   "cell_type": "code",
   "source": "df_train.columns\n",
   "id": "46d2b88c3f3a2822",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'label'], dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T12:17:53.578690Z",
     "start_time": "2025-11-10T12:17:53.567418Z"
    }
   },
   "cell_type": "code",
   "source": "df_test.columns\n",
   "id": "8effb28e45c472f3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'label'], dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T12:17:53.597226Z",
     "start_time": "2025-11-10T12:17:53.587052Z"
    }
   },
   "cell_type": "code",
   "source": "df_train['label'].unique()",
   "id": "a8ff46924f8575c2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T12:17:53.620104Z",
     "start_time": "2025-11-10T12:17:53.605001Z"
    }
   },
   "cell_type": "code",
   "source": "df_test['label'].unique()",
   "id": "b0ec880846bb46c0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T12:17:53.649276Z",
     "start_time": "2025-11-10T12:17:53.643387Z"
    }
   },
   "cell_type": "code",
   "source": "df_train['text'].sample(10)",
   "id": "e7dcacbc14885b77",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11413    Gadar is an example of one of Bollywood worst ...\n",
       "10323    Having seen men Behind the Sun I guess I hoped...\n",
       "6509     Let me start off by saying I am not a fan of h...\n",
       "919      The plot for Descent, if it actually can be ca...\n",
       "12605    If you fast forward through the horrible singi...\n",
       "21949    This film screened last night at Austin's Para...\n",
       "4050     After watching some of HBO's great stuff - Ban...\n",
       "4124     Given the people involved, it is hard to see w...\n",
       "17977    Hitokiri (which translates roughly as \"assassi...\n",
       "11410    A romanticised and thoroughly false vision of ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T12:17:53.714207Z",
     "start_time": "2025-11-10T12:17:53.709332Z"
    }
   },
   "cell_type": "code",
   "source": "df_test['text'].sample(10)",
   "id": "b6062f3f48dd106a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23313    I saw Bon voyage 2 days ago and I found it an ...\n",
       "17143    \"Sister Helen\" is a superb documentary about a...\n",
       "12074    Any evening with Jonathan Ross now means to me...\n",
       "3230     I'm one of those gluttons for punishment when ...\n",
       "9838     I don't care what anyone else says, this movie...\n",
       "22523    If you like Madonna or not, this movie is hila...\n",
       "2242     I went to see the movie because my boyfriend w...\n",
       "17650    He is very good in this role as a disaffected ...\n",
       "13850    Hey what do you expect form a very low budget ...\n",
       "19035    When Liv Ullman's character says, \"I feel like...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T12:17:53.848153Z",
     "start_time": "2025-11-10T12:17:53.843557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def calculate_max_sentence_length(df, text_column='text', percentile=95, verbose=True):\n",
    "    \"\"\"\n",
    "    Calculate the maximum sentence length based on a percentile.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe containing text data\n",
    "    text_column : str\n",
    "        Name of the column containing text (default: 'text')\n",
    "    percentile : int\n",
    "        Percentile to use for max length (default: 95)\n",
    "        Use 90 for more aggressive truncation, 99 for more conservative\n",
    "    verbose : bool\n",
    "        Whether to print statistics (default: True)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    int\n",
    "        The calculated max sentence length based on the percentile\n",
    "    \"\"\"\n",
    "    all_sentence_lengths = []\n",
    "\n",
    "    for text in df[text_column]:\n",
    "        # Split on multiple sentence-ending punctuation marks: . ? !\n",
    "        for sentence in re.split(r'[.!?]+', text):\n",
    "            word_count = len(sentence.split())\n",
    "            if word_count > 0:  # Only count non-empty sentences\n",
    "                all_sentence_lengths.append(word_count)\n",
    "\n",
    "    # Calculate the specified percentile\n",
    "    max_length = int(np.percentile(all_sentence_lengths, percentile))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"=== Sentence Length Statistics ===\")\n",
    "        print(f\"Total sentences analyzed: {len(all_sentence_lengths)}\")\n",
    "        print(f\"Maximum sentence length: {max(all_sentence_lengths)} words\")\n",
    "        print(f\"{percentile}th percentile length: {max_length} words\")\n",
    "        print(f\"Mean sentence length: {np.mean(all_sentence_lengths):.2f} words\")\n",
    "        print(f\"Median sentence length: {np.median(all_sentence_lengths):.2f} words\")\n",
    "        print(f\"\\nUsing {max_length} as max_length will truncate ~{100-percentile}% of sentences\")\n",
    "\n",
    "    return max_length"
   ],
   "id": "9666cdc311d3a395",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T12:17:53.889872Z",
     "start_time": "2025-11-10T12:17:53.883908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class pre_process_text(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        prep_sentences = []\n",
    "        for text in X:\n",
    "            # Remove HTML tags (good practice)\n",
    "            text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "            # Tokenize\n",
    "            tokens = word_tokenize(text)\n",
    "\n",
    "            words_to_keep = {\"not\", \"no\", \"against\", \"down\",\"no\", \"nor\", \"not\", \"don\",\"very\", \"too\", \"more\", \"most\", \"so\", \"only\"}\n",
    "\n",
    "\n",
    "            for word in words_to_keep:\n",
    "                self.stop_words.discard(word)\n",
    "\n",
    "            # Process tokens\n",
    "            processed = [\n",
    "                self.lemmatizer.lemmatize(token.lower())\n",
    "                for token in tokens\n",
    "                if token.isalpha() and token.lower() not in self.stop_words\n",
    "            ]\n",
    "\n",
    "            # Join back to a string\n",
    "            prep_sentences.append(\" \".join(processed))\n",
    "\n",
    "        return prep_sentences\n",
    "\n",
    "class Word2VecAverager(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, w2v_model):\n",
    "        self.w2v_model = w2v_model\n",
    "        self.vector_size = w2v_model.wv.vector_size\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        avg_vectors = []\n",
    "\n",
    "        for doc in X:\n",
    "            doc_vectors = []\n",
    "            # We .split() the processed string\n",
    "            for word in doc.split():\n",
    "                if word in self.w2v_model.wv:\n",
    "                    doc_vectors.append(self.w2v_model.wv[word])\n",
    "\n",
    "            if not doc_vectors:\n",
    "                avg_vectors.append(np.zeros(self.vector_size))\n",
    "            else:\n",
    "                avg_vectors.append(np.mean(doc_vectors, axis=0))\n",
    "\n",
    "        return np.array(avg_vectors)\n"
   ],
   "id": "d59225a3741ea4c",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T12:17:53.943608Z",
     "start_time": "2025-11-10T12:17:53.894814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_train = pd.DataFrame(df_train)\n",
    "df_test = pd.DataFrame(df_test)\n",
    "\n",
    "X_train = df_train['text']\n",
    "y_train = df_train['label'].values\n",
    "X_test = df_test['text']\n",
    "y_test = df_test['label'].values"
   ],
   "id": "de728b1170ec0dc2",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-10T12:17:53.949321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras.src.callbacks import EarlyStopping\n",
    "from keras.src.layers import Bidirectional, Dropout\n",
    "\n",
    "# --- Pre-process all text data ---\n",
    "print(\"--- Preprocessing Text ---\")\n",
    "pre_processor = pre_process_text()\n",
    "\n",
    "# Fit on training data and transform both train and test\n",
    "X_train = pre_processor.fit_transform(X_train)\n",
    "X_test = pre_processor.transform(X_test)\n",
    "print(f\"Example processed review: {X_train[0][:150]}...\")\n",
    "\n",
    "tokenized_processed_train = [review.split() for review in X_train]\n",
    "\n",
    "\n",
    "embedding_dim = 100\n",
    "min_word_count = 3\n",
    "window_size = 5\n",
    "num_workers = 4\n",
    "\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=tokenized_processed_train,\n",
    "    vector_size=embedding_dim,\n",
    "    window=window_size,\n",
    "    min_count=min_word_count,\n",
    "    workers=num_workers,\n",
    "    sg=1,       # Use Skip-gram\n",
    "    negative=5  # Use Negative Sampling\n",
    ")\n",
    "\n",
    "keras_tokenizer = Tokenizer()\n",
    "keras_tokenizer.fit_on_texts(X_train) # Fit on the processed text\n",
    "X_train_sequences = keras_tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "\n",
    "# all_lengths = [len(seq) for seq in X_train_sequences]\n",
    "# # max_length = int(np.percentile(all_lengths, 95))\n",
    "# max_length = max(len(sentence.split())\n",
    "#                 for text in df_train['text']\n",
    "#                 for sentence in text.split('.'))\n",
    "max_length = calculate_max_sentence_length(df_train)\n",
    "\n",
    "\n",
    "\n",
    "print(max_length)\n",
    "\n",
    "X_train_padded = pad_sequences(\n",
    "    X_train_sequences,\n",
    "    maxlen=max_length,\n",
    "    padding='post'\n",
    ")\n",
    "\n",
    "\n",
    "vocab_size = len(keras_tokenizer.word_index) + 1\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in keras_tokenizer.word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]\n",
    "\n",
    "\n",
    "def build_lstm_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=max_length,  # Use calculated max_length\n",
    "        trainable=False  # Freeze the embeddings\n",
    "    ))\n",
    "    # Use Bidirectional LSTM\n",
    "    model.add(Bidirectional(LSTM(\n",
    "        units=100,\n",
    "        recurrent_dropout=0.2  # Add recurrent dropout\n",
    "    )))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "early_stopper = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True  # Critically important!\n",
    ")\n",
    "n_splits = 5\n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "acc_scores, f1_scores, precision_scores, recall_scores = [], [], [], []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_padded, y_train)):\n",
    "    print(f\"\\n--- Fold {fold + 1}/{n_splits} ---\")\n",
    "    X_fold_train = X_train_padded[train_idx]\n",
    "    y_fold_train = y_train[train_idx]\n",
    "    X_fold_val = X_train_padded[val_idx]\n",
    "    y_fold_val = y_train[val_idx]\n",
    "\n",
    "    model = build_lstm_model()\n",
    "    print(f\"Training on {len(X_fold_train)} samples, validating on {len(X_fold_val)} samples.\")\n",
    "    model.fit(\n",
    "        X_fold_train, y_fold_train,\n",
    "        epochs=20,\n",
    "        callbacks=[early_stopper],\n",
    "        validation_data=(X_fold_val, y_fold_val),\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    y_pred_probs = model.predict(X_fold_val)\n",
    "    y_pred_classes = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "    acc = accuracy_score(y_fold_val, y_pred_classes)\n",
    "    precision = precision_score(y_fold_val, y_pred_classes, zero_division=0)\n",
    "    recall = recall_score(y_fold_val, y_pred_classes, zero_division=0)\n",
    "    f1 = f1_score(y_fold_val, y_pred_classes, zero_division=0)\n",
    "\n",
    "    acc_scores.append(acc)\n",
    "    f1_scores.append(f1)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "\n",
    "    print(f\"Fold {fold + 1} Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_fold_val, y_pred_classes, zero_division=0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "final_model = build_lstm_model()\n",
    "final_model.fit(\n",
    "    X_train_padded,  # All processed, padded training data\n",
    "    y_train,         # All training labels\n",
    "    epochs=20,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Evaluating on Test Set...\")\n",
    "X_test_sequences = keras_tokenizer.texts_to_sequences(X_test) # Use processed X_test\n",
    "X_test_padded = pad_sequences(\n",
    "    X_test_sequences,\n",
    "    maxlen=max_length, # Use same max_length\n",
    "    padding='post'\n",
    ")\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_pred_probs = final_model.predict(X_test_padded)\n",
    "test_pred_classes = (test_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "print(\"\\n--- Test Set Performance ---\")\n",
    "test_acc = accuracy_score(y_test, test_pred_classes)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(classification_report(y_test, test_pred_classes, zero_division=0))"
   ],
   "id": "6fbb8d0f55d12fa8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preprocessing Text ---\n",
      "Example processed review: rented video store controversy surrounded first released also heard first seized custom ever tried enter country therefore fan film considered controv...\n",
      "=== Sentence Length Statistics ===\n",
      "Total sentences analyzed: 334402\n",
      "Maximum sentence length: 306 words\n",
      "95th percentile length: 40 words\n",
      "Mean sentence length: 17.66 words\n",
      "Median sentence length: 16.00 words\n",
      "\n",
      "Using 40 as max_length will truncate ~5% of sentences\n",
      "40\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "Training on 20000 samples, validating on 5000 samples.\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
