{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T14:14:24.678414Z",
     "start_time": "2025-11-06T14:14:24.674913Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"hello\")",
   "id": "f5a15daaa61d77af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T14:24:37.932048Z",
     "start_time": "2025-11-06T14:24:35.930049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# --- 1. Example Data (Movie Reviews) ---\n",
    "# In a real project, you'd have thousands of these\n",
    "reviews = [\n",
    "    \"this movie was amazing and fantastic\",\n",
    "    \"I hated this movie it was boring\",\n",
    "    \"loved it so good\",\n",
    "    \"what a waste of time\"\n",
    "]\n",
    "# 0 = negative, 1 = positive\n",
    "labels = np.array([1, 0, 1, 0])\n",
    "\n",
    "# --- 2. Train Word2Vec (The \"Dictionary\") ---\n",
    "\n",
    "# Tokenize the text (split sentences into lists of words)\n",
    "tokenized_reviews = [review.split() for review in reviews]\n",
    "\n",
    "# Define Word2Vec model parameters\n",
    "embedding_dim = 50  # Each word will be a 50-dimensional vector\n",
    "min_word_count = 1  # Count words that appear at least once\n",
    "window_size = 2  # Look at 2 words to the left and 2 to the right\n",
    "\n",
    "# Train the Word2Vec model on our reviews\n",
    "print(\"Training Word2Vec model...\")\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=tokenized_reviews,\n",
    "    vector_size=embedding_dim,\n",
    "    window=window_size,\n",
    "    min_count=min_word_count\n",
    ")\n",
    "print(\"Word2Vec model trained.\")\n",
    "\n",
    "# --- 3. Prepare Data for LSTM (Tokenizing & Padding) ---\n",
    "\n",
    "# Keras needs integer sequences, not words.\n",
    "# We create a new Tokenizer to build an integer-to-word index.\n",
    "keras_tokenizer = Tokenizer()\n",
    "keras_tokenizer.fit_on_texts(reviews)\n",
    "\n",
    "# Convert reviews to sequences of integers\n",
    "sequences = keras_tokenizer.texts_to_sequences(reviews)\n",
    "\n",
    "# Get the vocabulary size from the Keras tokenizer\n",
    "vocab_size = len(keras_tokenizer.word_index) + 1  # +1 for the 0 padding\n",
    "\n",
    "# Pad sequences so they are all the same length\n",
    "max_length = 10  # Max length of a review (in words)\n",
    "padded_sequences = pad_sequences(\n",
    "    sequences,\n",
    "    maxlen=max_length,\n",
    "    padding='post'\n",
    ")\n",
    "\n",
    "print(\"\\nKeras Tokenizer Word Index:\", keras_tokenizer.word_index)\n",
    "print(\"\\nPadded Integer Sequences:\\n\", padded_sequences)\n",
    "\n",
    "# --- 4. Create the Embedding Matrix (Connecting Word2Vec to Keras) ---\n",
    "\n",
    "# We create a matrix where the i-th row is the Word2Vec vector\n",
    "# for the word with index 'i' in the Keras tokenizer.\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in keras_tokenizer.word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]\n",
    "    # Words not in the Word2Vec model (e.g., if min_count was > 1)\n",
    "    # will be left as all-zero vectors.\n",
    "\n",
    "print(f\"\\nEmbedding Matrix shape: {embedding_matrix.shape}\")\n",
    "\n",
    "# --- 5. Build the LSTM Model ---\n",
    "\n",
    "print(\"Building Keras LSTM model...\")\n",
    "model = Sequential()\n",
    "\n",
    "# Add the Embedding layer\n",
    "# This layer is our \"lookup table\" (Word2Vec)\n",
    "model.add(Embedding(\n",
    "    input_dim=vocab_size,  # Size of our vocabulary\n",
    "    output_dim=embedding_dim,  # Dimension of our vectors (from Word2Vec)\n",
    "    weights=[embedding_matrix],  # Pre-load the Word2Vec weights\n",
    "    input_length=max_length,  # Length of our padded sequences\n",
    "    trainable=False  # **Crucial: Freeze the embeddings!**\n",
    "    # We don't want to re-train them.\n",
    "))\n",
    "\n",
    "# Add the LSTM layer (The \"Thinker\")\n",
    "# It processes the sequences of vectors\n",
    "model.add(LSTM(units=100))  # 100 is the number of memory units\n",
    "\n",
    "# Add the final output layer\n",
    "# Sigmoid is used for binary (0 or 1) classification\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# --- 6. Train the LSTM Model ---\n",
    "\n",
    "print(\"\\nTraining LSTM model...\")\n",
    "# With a real dataset, you would use a validation_split\n",
    "model.fit(\n",
    "    padded_sequences,\n",
    "    labels,\n",
    "    epochs=20,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- 7. Make a Prediction ---\n",
    "\n",
    "print(\"\\nMaking a prediction...\")\n",
    "test_review = \"this movie was great and good\"\n",
    "test_seq = keras_tokenizer.texts_to_sequences([test_review])\n",
    "test_pad = pad_sequences(test_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "prediction = model.predict(test_pad)\n",
    "print(f\"Review: '{test_review}'\")\n",
    "print(f\"Prediction (Raw): {prediction[0][0]}\")\n",
    "print(f\"Predicted Label: {'Positive' if prediction[0][0] > 0.5 else 'Negative'}\")\n"
   ],
   "id": "e950a2d91f5c07f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model...\n",
      "Word2Vec model trained.\n",
      "\n",
      "Keras Tokenizer Word Index: {'this': 1, 'movie': 2, 'was': 3, 'it': 4, 'amazing': 5, 'and': 6, 'fantastic': 7, 'i': 8, 'hated': 9, 'boring': 10, 'loved': 11, 'so': 12, 'good': 13, 'what': 14, 'a': 15, 'waste': 16, 'of': 17, 'time': 18}\n",
      "\n",
      "Padded Integer Sequences:\n",
      " [[ 1  2  3  5  6  7  0  0  0  0]\n",
      " [ 8  9  1  2  4  3 10  0  0  0]\n",
      " [11  4 12 13  0  0  0  0  0  0]\n",
      " [14 15 16 17 18  0  0  0  0  0]]\n",
      "\n",
      "Embedding Matrix shape: (19, 50)\n",
      "Building Keras LSTM model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_7\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_7 (\u001B[38;5;33mEmbedding\u001B[0m)         │ ?                      │           \u001B[38;5;34m950\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_7 (\u001B[38;5;33mLSTM\u001B[0m)                   │ ?                      │   \u001B[38;5;34m0\u001B[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001B[38;5;33mDense\u001B[0m)                 │ ?                      │   \u001B[38;5;34m0\u001B[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">950</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m950\u001B[0m (3.71 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">950</span> (3.71 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m950\u001B[0m (3.71 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">950</span> (3.71 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LSTM model...\n",
      "Epoch 1/20\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 970ms/step - accuracy: 0.5000 - loss: 0.6930\n",
      "Epoch 2/20\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - accuracy: 0.5000 - loss: 0.6926\n",
      "Epoch 3/20\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 40ms/step - accuracy: 0.7500 - loss: 0.6922\n",
      "Epoch 4/20\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - accuracy: 0.5000 - loss: 0.6918\n",
      "Epoch 5/20\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - accuracy: 0.7500 - loss: 0.6914\n",
      "Epoch 6/20\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 39ms/step - accuracy: 1.0000 - loss: 0.6909\n",
      "Epoch 7/20\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step - accuracy: 1.0000 - loss: 0.6904\n",
      "Epoch 8/20\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 40ms/step - accuracy: 1.0000 - loss: 0.6898\n",
      "Epoch 9/20\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 47ms/step - accuracy: 1.0000 - loss: 0.6891\n",
      "Epoch 10/20\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 46ms/step - accuracy: 1.0000 - loss: 0.6883\n",
      "Epoch 11/20\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - accuracy: 1.0000 - loss: 0.6874\n",
      "Epoch 12/20\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - accuracy: 1.0000 - loss: 0.6863\n",
      "Epoch 13/20\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - accuracy: 1.0000 - loss: 0.6850\n",
      "Epoch 14/20\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 40ms/step - accuracy: 1.0000 - loss: 0.6834\n",
      "Epoch 15/20\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 39ms/step - accuracy: 1.0000 - loss: 0.6816\n",
      "Epoch 16/20\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - accuracy: 1.0000 - loss: 0.6794\n",
      "Epoch 17/20\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 39ms/step - accuracy: 1.0000 - loss: 0.6769\n",
      "Epoch 18/20\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step - accuracy: 1.0000 - loss: 0.6738\n",
      "Epoch 19/20\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 40ms/step - accuracy: 1.0000 - loss: 0.6702\n",
      "Epoch 20/20\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step - accuracy: 1.0000 - loss: 0.6658\n",
      "\n",
      "Making a prediction...\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 111ms/step\n",
      "Review: 'this movie was great and good'\n",
      "Prediction (Raw): 0.5130468010902405\n",
      "Predicted Label: Positive\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d30f06ca21de13c3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
